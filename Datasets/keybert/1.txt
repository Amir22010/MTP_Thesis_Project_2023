https://maartengr.github.io/KeyBERT/api/keybert.html
__init__(self, model='all-MiniLM-L6-v2')

keybert-0.7.0 sentence-transformers-2.2.2
https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
sentiment
https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1


base model - https://huggingface.co/allenai/tk-instruct-base-def-pos
#finetuned model - https://huggingface.co/kevinscaria/joint_tk-instruct-base-def-pos-neg-neut-combined

text generative model
https://huggingface.co/docs/transformers/v4.30.0/main_classes/text_generation

to get the generated text probability score
https://github.com/huggingface/transformers/issues/10012
https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075/27

training batch size = 4 GPU P100 [16 GB gpu memory, 13 GB Ram]
prediction batch size=16


applying on pyabsa English model checkpoint annotation hospital review data - 

w&b - golden-feather-21

training data 16956
validation data 3239
testing data 1000

number of aspects and its polarity increases
training with more epoch is must
double the epoch
8 epoch
batch size reduces to 4
fp16 training

combining two models into one model capability
increasing label length will reduce the testing accuracy little bit


Epoch	Training Loss	Validation Loss
1	0.150800	0.112104
2	0.103700	0.074400
3	0.074500	0.065034
4	0.065900	0.062676
5	0.059700	0.058017
6	0.050700	0.056635
7	0.046500	0.056589
8	0.046300	0.056924

processing time for prediction increases by 2x
45 mins for training data prediction
Train Precision:  0.9552227554840818
Train Recall:  0.9552107544443746
Train F1:  0.9552167549265339
Test Precision:  0.9300919842312746
Test Recall:  0.9301530981010578
Test F1:  0.9301225401622918
UnseenTest Precision:  0.9329632792485055
UnseenTest Recall:  0.9329632792485055
UnseenTest F1:  0.9329632792485054